{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "f4e29423",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import stealth_requests as requests\n",
    "import requests as r\n",
    "from bs4 import BeautifulSoup\n",
    "from markdownify import markdownify as md\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "from time import sleep\n",
    "import base64\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "3e94ab9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "lightrag_url = os.getenv('LIGHTRAG_URL', 'http://localhost:9621')\n",
    "\n",
    "with open('data/urls.txt', 'r') as f:\n",
    "    page_urls = [line.strip() for line in f if line.strip()]\n",
    "\n",
    "with open('data/pdfs.txt', 'r') as f:\n",
    "    pdf_urls = [line.strip() for line in f if line.strip()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "5604aacb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_content(url):\n",
    "    resp = requests.get(url)\n",
    "    soup = BeautifulSoup(resp.content, 'html.parser')\n",
    "\n",
    "    # Find the 'div' with class 'content-wrap'\n",
    "    content_wrap_div = soup.find('div', class_='content-wrap')\n",
    "\n",
    "    # Get the HTML content of this div\n",
    "    if content_wrap_div:\n",
    "        content_wrap_html = str(content_wrap_div)\n",
    "        md_content = md(content_wrap_html)\n",
    "        #print(md_content)\n",
    "        return md_content\n",
    "    else:\n",
    "        print(f\"Div with class 'content-wrap' not found in {url}\")\n",
    "        return None\n",
    "\n",
    "def insert_page_to_rag(page):\n",
    "    data = {\n",
    "        \"text\": page[\"content\"],\n",
    "        \"file_path\": page[\"url\"],\n",
    "        \"metadata\": {\n",
    "            \"url\": page[\"url\"]\n",
    "        },\n",
    "        \"source\": \"web\"\n",
    "    }\n",
    "    resp = r.post(f\"{lightrag_url}/documents/text\", json=data)\n",
    "    return resp.status_code\n",
    "\n",
    "def insert_pdf_to_rag(pdf_url):\n",
    "    resp = requests.get(pdf_url)\n",
    "    if resp.status_code == 200:\n",
    "        filename = pdf_url.split('/')[-1] or \"file.pdf\"\n",
    "        files = {\n",
    "            'file': (filename, resp.content, 'application/pdf')\n",
    "        }\n",
    "        headers = {\n",
    "            'accept': 'application/json'\n",
    "            # Do not set Content-Type, requests will set it for multipart/form-data\n",
    "        }\n",
    "        upload_resp = r.post(f\"{lightrag_url}/documents/file\", files=files, headers=headers)\n",
    "        return upload_resp\n",
    "    else:\n",
    "        print(f\"Failed to download PDF: {pdf_url}\")\n",
    "        return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a95546ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/136 [00:15<?, ?it/s]\n",
      "Successfully inserted PDF: https://summer.skku.edu/_res/summer/etc/2025_ebrochure.pdf: 100%|██████████| 1/1 [00:00<00:00,  1.06it/s]\n",
      "Successfully inserted PDF: https://summer.skku.edu/_res/summer/etc/2025_ebrochure.pdf: 100%|██████████| 1/1 [00:00<00:00,  1.06it/s]\n"
     ]
    }
   ],
   "source": [
    "pdf_pbar = tqdm(pdf_urls)\n",
    "\n",
    "for url in pdf_pbar:\n",
    "    pdf_pbar.set_description(f\"Processing PDF: {url}\")\n",
    "    status_code = insert_pdf_to_rag(url)\n",
    "    if status_code.status_code == 200:\n",
    "        pdf_pbar.set_description(f\"Successfully inserted PDF: {url}\")\n",
    "    else:\n",
    "        print(f\"Failed to insert PDF: {url}, Status Code: {status_code.content}\")\n",
    "    sleep(0.5)  # Sleep to avoid overwhelming the server"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f33b862",
   "metadata": {},
   "outputs": [],
   "source": [
    "url_pbar = tqdm(page_urls)\n",
    "\n",
    "markdown_dicts = []\n",
    "for url in url_pbar:\n",
    "    url_pbar.set_description(f\"Scraping {url}\")\n",
    "    content = scrape_content(url)\n",
    "    if content:\n",
    "        markdown_dicts.append({\n",
    "            \"url\": url,\n",
    "            \"content\": content\n",
    "        })\n",
    "    sleep(0.5)  # To avoid overwhelming the server\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
