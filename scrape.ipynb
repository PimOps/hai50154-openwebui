{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "f4e29423",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import stealth_requests as requests\n",
    "import requests as r\n",
    "from bs4 import BeautifulSoup\n",
    "from markdownify import markdownify as md\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "from time import sleep\n",
    "import base64\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "3e94ab9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "lightrag_url = os.getenv('LIGHTRAG_URL', 'http://localhost:9621')\n",
    "\n",
    "with open('data/urls.txt', 'r') as f:\n",
    "    page_urls = [line.strip() for line in f if line.strip()]\n",
    "\n",
    "with open('data/pdfs.txt', 'r') as f:\n",
    "    pdf_urls = [line.strip() for line in f if line.strip()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "5604aacb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_content(url):\n",
    "    resp = requests.get(url)\n",
    "    soup = BeautifulSoup(resp.content, 'html.parser')\n",
    "\n",
    "    # Find the 'div' with class 'content-wrap'\n",
    "    content_wrap_div = soup.find('div', class_='content-wrap')\n",
    "\n",
    "    # Get the HTML content of this div\n",
    "    if content_wrap_div:\n",
    "        content_wrap_html = str(content_wrap_div)\n",
    "        md_content = md(content_wrap_html)\n",
    "        #print(md_content)\n",
    "        return md_content\n",
    "    else:\n",
    "        print(f\"Div with class 'content-wrap' not found in {url}\")\n",
    "        return None\n",
    "\n",
    "def insert_page_to_rag(page):\n",
    "    data = {\n",
    "        \"text\": page[\"content\"],\n",
    "        \"file_path\": page[\"url\"],\n",
    "        \"metadata\": {\n",
    "            \"url\": page[\"url\"]\n",
    "        },\n",
    "        \"source\": \"web\"\n",
    "    }\n",
    "    resp = r.post(f\"{lightrag_url}/documents/text\", json=data)\n",
    "    return resp.status_code\n",
    "\n",
    "def insert_pdf_to_rag(pdf_url):\n",
    "    resp = requests.get(pdf_url)\n",
    "    if resp.status_code == 200:\n",
    "        filename = pdf_url.split('/')[-1] or \"file.pdf\"\n",
    "        files = {\n",
    "            'file': (filename, resp.content, 'application/pdf')\n",
    "        }\n",
    "        headers = {\n",
    "            'accept': 'application/json'\n",
    "            # Do not set Content-Type, requests will set it for multipart/form-data\n",
    "        }\n",
    "        upload_resp = r.post(f\"{lightrag_url}/documents/file\", files=files, headers=headers)\n",
    "        return upload_resp\n",
    "    else:\n",
    "        print(f\"Failed to download PDF: {pdf_url}\")\n",
    "        return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a95546ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing PDF: https://summer.skku.edu/_res/summer/etc/2025_ebrochure.pdf\n",
      "Failed to insert PDF: https://summer.skku.edu/_res/summer/etc/2025_ebrochure.pdf, Status Code: b'{\"status\":\"success\",\"message\":\"File \\'2025_ebrochure.pdf\\' saved successfully. Processing will continue in background.\"}'\n",
      "Failed to insert PDF: https://summer.skku.edu/_res/summer/etc/2025_ebrochure.pdf, Status Code: b'{\"status\":\"success\",\"message\":\"File \\'2025_ebrochure.pdf\\' saved successfully. Processing will continue in background.\"}'\n"
     ]
    }
   ],
   "source": [
    "pdf_pbar = tqdm(pdf_urls)\n",
    "\n",
    "for url in pdf_pbar:\n",
    "    pdf_pbar.set_description(f\"Processing PDF: {url}\")\n",
    "    status_code = insert_pdf_to_rag(url)\n",
    "    if status_code == 200:\n",
    "        print(f\"Successfully inserted PDF: {url}\")\n",
    "    else:\n",
    "        print(f\"Failed to insert PDF: {url}, Status Code: {status_code.content}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f33b862",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scraping h:   0%|          | 0/50 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "DNSError",
     "evalue": "Failed to perform, curl: (6) Could not resolve host: h. See https://curl.se/libcurl/c/libcurl-errors.html first for more details.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mCurlError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.11/site-packages/curl_cffi/requests/session.py:600\u001b[39m, in \u001b[36mSession.request\u001b[39m\u001b[34m(self, method, url, params, data, json, headers, cookies, files, auth, timeout, allow_redirects, max_redirects, proxies, proxy, proxy_auth, verify, referer, accept_encoding, content_callback, impersonate, ja3, akamai, extra_fp, default_headers, default_encoding, quote, http_version, interface, cert, stream, max_recv_speed, multipart)\u001b[39m\n\u001b[32m    599\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m600\u001b[39m         \u001b[43mc\u001b[49m\u001b[43m.\u001b[49m\u001b[43mperform\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    601\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m CurlError \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.11/site-packages/curl_cffi/curl.py:329\u001b[39m, in \u001b[36mCurl.perform\u001b[39m\u001b[34m(self, clear_headers)\u001b[39m\n\u001b[32m    328\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m329\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_check_error\u001b[49m\u001b[43m(\u001b[49m\u001b[43mret\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mperform\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    330\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    331\u001b[39m     \u001b[38;5;66;03m# cleaning\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.11/site-packages/curl_cffi/curl.py:154\u001b[39m, in \u001b[36mCurl._check_error\u001b[39m\u001b[34m(self, errcode, *args)\u001b[39m\n\u001b[32m    153\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m error \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m154\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m error\n",
      "\u001b[31mCurlError\u001b[39m: Failed to perform, curl: (6) Could not resolve host: h. See https://curl.se/libcurl/c/libcurl-errors.html first for more details.",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[31mDNSError\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[34]\u001b[39m\u001b[32m, line 7\u001b[39m\n\u001b[32m      5\u001b[39m pbar.set_description(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mScraping \u001b[39m\u001b[38;5;132;01m{\u001b[39;00murl\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m      6\u001b[39m url = url\n\u001b[32m----> \u001b[39m\u001b[32m7\u001b[39m content = \u001b[43mscrape_content\u001b[49m\u001b[43m(\u001b[49m\u001b[43murl\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      8\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m content:\n\u001b[32m      9\u001b[39m     markdown_dicts.append({\n\u001b[32m     10\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33murl\u001b[39m\u001b[33m\"\u001b[39m: url,\n\u001b[32m     11\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mcontent\u001b[39m\u001b[33m\"\u001b[39m: content\n\u001b[32m     12\u001b[39m     })\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[33]\u001b[39m\u001b[32m, line 2\u001b[39m, in \u001b[36mscrape_content\u001b[39m\u001b[34m(url)\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mscrape_content\u001b[39m(url):\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m     resp = \u001b[43mrequests\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43murl\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      3\u001b[39m     soup = BeautifulSoup(resp.content, \u001b[33m'\u001b[39m\u001b[33mhtml.parser\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m      5\u001b[39m     \u001b[38;5;66;03m# Find the 'div' with class 'content-wrap'\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.11/site-packages/stealth_requests/__init__.py:8\u001b[39m, in \u001b[36mrequest\u001b[39m\u001b[34m(method, url, *args, **kwargs)\u001b[39m\n\u001b[32m      6\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mrequest\u001b[39m(method: \u001b[38;5;28mstr\u001b[39m, url: \u001b[38;5;28mstr\u001b[39m, *args, **kwargs) -> StealthResponse:\n\u001b[32m      7\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m StealthSession() \u001b[38;5;28;01mas\u001b[39;00m s:\n\u001b[32m----> \u001b[39m\u001b[32m8\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43ms\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.11/site-packages/stealth_requests/session.py:137\u001b[39m, in \u001b[36mStealthSession.request\u001b[39m\u001b[34m(self, method, url, *args, **kwargs)\u001b[39m\n\u001b[32m    135\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mrequest\u001b[39m(\u001b[38;5;28mself\u001b[39m, method: \u001b[38;5;28mstr\u001b[39m, url: \u001b[38;5;28mstr\u001b[39m, *args, **kwargs) -> Response:\n\u001b[32m    136\u001b[39m     headers = \u001b[38;5;28mself\u001b[39m.get_dynamic_headers(url) | kwargs.pop(\u001b[33m'\u001b[39m\u001b[33mheaders\u001b[39m\u001b[33m'\u001b[39m, {})\n\u001b[32m--> \u001b[39m\u001b[32m137\u001b[39m     resp = \u001b[43mSession\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m=\u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    138\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m StealthResponse(resp)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.11/site-packages/curl_cffi/requests/session.py:605\u001b[39m, in \u001b[36mSession.request\u001b[39m\u001b[34m(self, method, url, params, data, json, headers, cookies, files, auth, timeout, allow_redirects, max_redirects, proxies, proxy, proxy_auth, verify, referer, accept_encoding, content_callback, impersonate, ja3, akamai, extra_fp, default_headers, default_encoding, quote, http_version, interface, cert, stream, max_recv_speed, multipart)\u001b[39m\n\u001b[32m    603\u001b[39m     rsp.request = req\n\u001b[32m    604\u001b[39m     error = code2error(e.code, \u001b[38;5;28mstr\u001b[39m(e))\n\u001b[32m--> \u001b[39m\u001b[32m605\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m error(\u001b[38;5;28mstr\u001b[39m(e), e.code, rsp) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01me\u001b[39;00m\n\u001b[32m    606\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    607\u001b[39m     rsp = \u001b[38;5;28mself\u001b[39m._parse_response(c, buffer, header_buffer, default_encoding)\n",
      "\u001b[31mDNSError\u001b[39m: Failed to perform, curl: (6) Could not resolve host: h. See https://curl.se/libcurl/c/libcurl-errors.html first for more details."
     ]
    }
   ],
   "source": [
    "url_pbar = tqdm(page_urls)\n",
    "\n",
    "markdown_dicts = []\n",
    "for url in url_pbar:\n",
    "    url_pbar.set_description(f\"Scraping {url}\")\n",
    "    content = scrape_content(url)\n",
    "    if content:\n",
    "        markdown_dicts.append({\n",
    "            \"url\": url,\n",
    "            \"content\": content\n",
    "        })\n",
    "    sleep(0.5)  # To avoid overwhelming the server\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "93566d8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "for page in markdown_dicts:\n",
    "    print(insert_page_to_rag(page))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "0eabda72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping https://summer.skku.edu/summer/index.do...\n",
      "Div with class 'content-wrap' not found.\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'encode'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[35]\u001b[39m\u001b[32m, line 8\u001b[39m\n\u001b[32m      6\u001b[39m url = url\n\u001b[32m      7\u001b[39m content = scrape_content(url)\n\u001b[32m----> \u001b[39m\u001b[32m8\u001b[39m markdown_bytes = \u001b[43mcontent\u001b[49m\u001b[43m.\u001b[49m\u001b[43mencode\u001b[49m(\u001b[33m'\u001b[39m\u001b[33mutf-8\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m      9\u001b[39m base64_bytes = base64.b64encode(markdown_bytes)\n\u001b[32m     10\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m content:\n",
      "\u001b[31mAttributeError\u001b[39m: 'NoneType' object has no attribute 'encode'"
     ]
    }
   ],
   "source": [
    "import base64\n",
    "\n",
    "markdown_dicts = []\n",
    "for url in page_urls:\n",
    "    print(f\"Scraping {url}...\")\n",
    "    url = url\n",
    "    content = scrape_content(url)\n",
    "    markdown_bytes = content.encode('utf-8')\n",
    "    base64_bytes = base64.b64encode(markdown_bytes)\n",
    "    if content:\n",
    "        markdown_dicts.append({\n",
    "            \"url\": url,\n",
    "            \"content\": content,\n",
    "            \"base64\": base64_bytes.decode('utf-8')\n",
    "        })\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ece5f5c5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "698f82d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "api_url = \"http://localhost:8080/\"\n",
    "api_key = os.getenv(\"OPEN_WEBUI_API_KEY\")\n",
    "headers = {\n",
    "    \"Authorization\": f\"Bearer {api_key}\",\n",
    "    \"Content-Type\": \"application/json\",\n",
    "    'Accept': 'application/json'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f989457e",
   "metadata": {},
   "outputs": [],
   "source": [
    "knowledge_base_data = {\n",
    "    \"name\": \"ISS Info\",\n",
    "    \"description\": \"Knowledge Base with information about International Summer Semester\",\n",
    "    \"data\": {},\n",
    "    \"access_control\": {}\n",
    "}\n",
    "\n",
    "# The api_url is defined in cell 4\n",
    "create_kb_url = f\"{api_url}api/v1/knowledge/create\"\n",
    "response = r.post(create_kb_url, headers=headers, json=knowledge_base_data)\n",
    "if response.status_code == 200:\n",
    "    kb_creation_response = response.json()\n",
    "    kb_creation_id = kb_creation_response.get(\"id\")\n",
    "    print(kb_creation_response)\n",
    "else:\n",
    "    print(f\"Failed to create knowledge base: {response.status_code} - {response.text}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86a9f54b",
   "metadata": {},
   "outputs": [],
   "source": [
    "kb_creation_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "825099e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "api_add_file_url = \"/api/v1/files/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "410fb1d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_file_to_kb(source_url, base64_content):\n",
    "    api_add_file_url = \"/api/v1/files/\"\n",
    "    files = {'file': base64_content}\n",
    "    response = r.post(f\"{api_url}{api_add_file_url}\", headers=headers, files=files)\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f0ded45",
   "metadata": {},
   "outputs": [],
   "source": [
    "add_file_to_kb"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
